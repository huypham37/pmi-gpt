# Conduct Search Engine Discovery Reconnaissance for Information Leakage

---
id: WSTG-INFO-01
tag: TA
---

## Brief Summary

In order for search engines to work, computer programs (or `robots`) regularly fetch data (referred to as [crawling](https://en.wikipedia.org/wiki/Web_crawler) from billions of pages on the web. These programs find web content and functionality by following links from other pages, or by looking at sitemaps. If a website uses a special file called `robots.
